{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b3a074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T08:31:42.318976Z",
     "start_time": "2021-05-15T08:31:40.356402Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import model_selection as ms\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e11a657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T08:31:42.643548Z",
     "start_time": "2021-05-15T08:31:42.320458Z"
    }
   },
   "outputs": [],
   "source": [
    "train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd58fb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T08:31:42.648798Z",
     "start_time": "2021-05-15T08:31:42.645363Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_data = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a72a5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.337Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = fetch_openml('mnist_784')\n",
    "x = np.array(dataset['data'])\n",
    "y = np.array(dataset['target'], dtype='int32')\n",
    "x_train, x_test, y_train, y_test = ms.train_test_split(x, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301ec99",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.341Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(784, 128)\n",
    "        self.l2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = F.relu(self.l1(input))\n",
    "        return F.log_softmax(self.l2(input), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2eddad",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.345Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "net = Net()\n",
    "opt = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    i = 0\n",
    "    for i in range(y_train.shape[0]//batch_size):\n",
    "        low = i*batch_size\n",
    "        x = torch.Tensor(x_train[low: low + batch_size]).float()\n",
    "        y = torch.Tensor(y_train[low: low + batch_size]).long()\n",
    "        opt.zero_grad()\n",
    "        output = net(x)\n",
    "        #  https://discuss.pytorch.org/t/does-nllloss-handle-log-softmax-and-softmax-in-the-same-way/8835\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e60e9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.351Z"
    }
   },
   "outputs": [],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad2fa2b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.356Z"
    }
   },
   "outputs": [],
   "source": [
    "layer = net.l1.state_dict()\n",
    "print(layer['weight'])\n",
    "print(layer['bias'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c836b13",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.359Z"
    }
   },
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(y_train.shape[0]//batch_size):\n",
    "        low = i*batch_size\n",
    "        x = torch.Tensor(x_train[low: low + batch_size]).float()\n",
    "        y = torch.Tensor(y_train[low: low + batch_size]).long()\n",
    "        output = net(x)\n",
    "        correct += int(torch.sum(y == torch.argmax(output, axis = 1)))\n",
    "        total += batch_size\n",
    "\n",
    "print(f\"Accuracy: {correct/total * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71af179",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.362Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for i in range(y_train.shape[0]//batch_size):\n",
    "        low = i*batch_size\n",
    "        x = torch.Tensor(x_train[low: low + batch_size]).float()\n",
    "        y = torch.Tensor(y_train[low: low + batch_size]).long()\n",
    "        output = net(x)\n",
    "        outputs.extend(list(torch.max(output, 1)[0]))\n",
    "outputs = np.array(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22ae19",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.365Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_list = sorted(list(zip(outputs, range(outputs.shape[0]))))\n",
    "x_bad = x_train[[x[1] for x in sorted_list[:64]]].reshape(8, 28*8, 28)\n",
    "\n",
    "# let's take a look at the most weird image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(np.concatenate(x_bad, axis=1), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a458cff6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.367Z"
    }
   },
   "outputs": [],
   "source": [
    "# References\n",
    "# https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\n",
    "# https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
    "# https://stackoverflow.com/questions/65192475/pytorch-logsoftmax-vs-softmax-for-crossentropyloss\n",
    "# https://stats.stackexchange.com/questions/223256/does-the-cross-entropy-cost-make-sense-in-the-context-of-regression#:~:text=In%20general%2C%20you%20can%20define,to%20a%20squared%20Mahalanobis%20distance.&text=So%20yes%2C%20cross%2Dentropy%20can%20be%20used%20for%20regression.\n",
    "# https://stackoverflow.com/questions/35304393/trying-to-understand-code-that-computes-the-gradient-wrt-to-the-input-for-logsof\n",
    "# https://cs231n.github.io/optimization-2/\n",
    "# https://forums.fast.ai/t/05-pet-breeds-understanding-nll-loss-function/84873/3\n",
    "# https://www.kaggle.com/dansbecker/what-is-log-loss\n",
    "# https://stackoverflow.com/questions/20027598/why-should-weights-of-neural-networks-be-initialized-to-random-numbers\n",
    "# https://www.deeplearning.ai/ai-notes/initialization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d9d81",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.370Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_layer(x, y):\n",
    "    # TODO: use a better way for initializing weights\n",
    "    # he - 92%\n",
    "    # return np.random.randn(x, y) * np.sqrt(2./x)\n",
    "    # xavier - 93%\n",
    "    # return np.random.randn(x, y) * np.sqrt(1./x)\n",
    "    # uniform - 97-98%\n",
    "    weights = np.random.uniform(-1., 1., size=(x,y))/np.sqrt(x*y)\n",
    "    bias = np.zeros((1, y))\n",
    "    return weights, bias\n",
    "    \n",
    "def relu(input):\n",
    "    return np.maximum(input, 0)\n",
    "\n",
    "\"\"\"\n",
    "def log_softmax(x):\n",
    "    # invalid value encountered in multiply warning due to underflow/overflow\n",
    "    sumx = np.sum(np.exp(x), axis=1).reshape(-1, 1)\n",
    "    return x-np.log(sumx)\n",
    "\"\"\"\n",
    "\n",
    "def log_softmax(x):\n",
    "    # http://gregorygundersen.com/blog/2020/02/09/log-sum-exp/\n",
    "    # https://stackoverflow.com/questions/61567597/how-is-log-softmax-implemented-to-compute-its-value-and-gradient-with-better\n",
    "    # https://www.youtube.com/watch?v=pZbkVup7fYE\n",
    "    # https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/\n",
    "    # https://blog.feedly.com/tricks-of-the-trade-logsumexp/\n",
    "    c = x.max(axis=1)\n",
    "    return x - (c + np.log(np.exp(x-c.reshape((-1, 1))).sum(axis=1))).reshape((-1, 1))\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db0e25",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.372Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    x_l1 = x.dot(l1) + b1\n",
    "    x_relu = relu(x_l1)\n",
    "    x_l2 = x_relu.dot(l2) + b2\n",
    "    y_pred = log_softmax(x_l2)\n",
    "    return y_pred\n",
    "\n",
    "def forward_backward(x, y):\n",
    "    \"\"\"\n",
    "    Some dimensional analysis:\n",
    "    x = input image of shape: batch_size * 784\n",
    "    y = true output for the images of shape: batch_size * 10\n",
    "    l1 = weights of layer 1 w shape: 784 * 128\n",
    "    l2 = weights of layer 2 w shape: 128 * 10\n",
    "    b1 = bias of layer 1 w shape: 1 * 128\n",
    "    b2 = bias of layer 1 w shape: 1 * 10\n",
    "    \n",
    "    x_l1 = the dot product of x and l1 of shape: batch_size * 128\n",
    "    x_relu = the ReLU activation function applied on x_l1 of shape: batch_size * 128\n",
    "    x_l2 = the dot product of x_relu and l2 of shape: batch_size * 10\n",
    "    y_pred = the log-softmax function applied on x_l2 of shape: batch_size * 10\n",
    "    y_mod = one-hot encoding of y of shape: batch_size * 10\n",
    "    y_pred = output of the neural network of shape: batch_size * 10\n",
    "    loss = loss of the network of shape: batch_size\n",
    "    \n",
    "    d_ypred = derivative of loss WRT y_pred of shape: batch_size * 10\n",
    "    d_lsm = derivative of the y_pred WRT x_l2 of shape: batch_size * 10\n",
    "    d_l2 = derivative of x_l2 WRT l2 of shape: 128 * 10\n",
    "    \n",
    "    d_relu = derivative of loss WRT x_l2 of shape: batch_size * 128\n",
    "    d_xl1 = derivative of ReLU WRT to x_l1 of shape: batch_size * 128\n",
    "    d_l1 = derivative of x_l1 WRT l1 of shape: 784 * 128\n",
    "    \n",
    "    d_l2 = derivative of nll_loss * derivative of lsm * x_relu of shape: 128 * 10\n",
    "    d_l1 = derivative of y_pred wrt x_relu * derivative of x_relu * x of shape: 784 * 128\n",
    "    \"\"\"\n",
    "    y_mod = np.zeros((len(y), 10))\n",
    "    y_mod[range(len(y)), y] = 1\n",
    "    \n",
    "    x_l1 = x.dot(l1) + b1\n",
    "    x_relu = relu(x_l1)\n",
    "    x_l2 = x_relu.dot(l2) + b2\n",
    "    y_pred = log_softmax(x_l2)\n",
    "    loss = (-y_mod * y_pred).mean(axis=1)\n",
    "    \n",
    "    # derivative of loss wrt y_pred\n",
    "    d_ypred = -(y_mod / len(y))\n",
    "    # derivative of log-softmax\n",
    "    d_lsm = d_ypred - (np.exp(y_pred) * d_ypred.sum(axis=1).reshape((-1, 1)))\n",
    "    # derivative of layer 2 weights\n",
    "    d_l2 = x_relu.T.dot(d_lsm)\n",
    "    d_b2 = d_lsm.mean(axis=0)\n",
    "    \n",
    "    # derivative of x_l2 wrt to loss\n",
    "    d_relu = d_lsm.dot(l2.T)\n",
    "    # derivative of relu\n",
    "    d_xl1 = (x_relu > 0).astype('float32') * d_relu\n",
    "    # derivative of layer 1 weights\n",
    "    d_l1 = x.T.dot(d_xl1)\n",
    "    d_b1 = d_xl1.mean(axis=0)\n",
    "    \n",
    "    return y_pred, d_l1, d_l2, d_b1, d_b2, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3720ba5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.375Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "l1, b1 = init_layer(28*28, 128)\n",
    "l2, b2 = init_layer(128, 10)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in trange(EPOCHS):\n",
    "    for i in range(1000):\n",
    "        idx = np.random.randint(0, x_train.shape[0], size=batch_size)\n",
    "        x = x_train[idx]\n",
    "        y = y_train[idx]\n",
    "        y_pred, d_l1, d_l2, d_b1, d_b2, loss = forward_backward(x, y)\n",
    "        l1 -= learning_rate * d_l1\n",
    "        l2 -= learning_rate * d_l2\n",
    "        b1 -= learning_rate * d_b1\n",
    "        b2 -= learning_rate * d_b2\n",
    "        losses.append(loss.mean())\n",
    "        accuracy = (y == np.argmax(y_pred, axis=1)).mean()\n",
    "        accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa060b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.378Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.ylim(-0.1, 1.1)\n",
    "plt.plot(losses)\n",
    "plt.plot(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ed591",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-15T08:31:40.381Z"
    }
   },
   "outputs": [],
   "source": [
    "out = forward(x_test)\n",
    "out = np.argmax(out, axis=1)\n",
    "print((y_test == out).mean() * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 200,
   "position": {
    "height": "754px",
    "left": "110px",
    "right": "20px",
    "top": "96px",
    "width": "753px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
